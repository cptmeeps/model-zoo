{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c8c87a-280c-4e2d-a5e2-b5b84a668508",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6831eb43-8979-4bc0-936a-3386c92ae175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('workspace/model-zoo/llava')\n",
    "\n",
    "import os, math, time, random, json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "import torch.autograd.profiler as profiler\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from clip import load\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='log.txt', level=logging.INFO, filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20d0ed-17a1-4e9a-a218-bf31af0abd34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c096202-28e7-4a4a-a778-fe917982c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "  dim: int = 4096\n",
    "  n_layers: int = 32\n",
    "  n_heads: int = 32\n",
    "  n_kv_heads: Optional[int] = None\n",
    "  vocab_size: int = 32000\n",
    "  multiple_of: int = 256  \n",
    "  ffn_dim_multiplier: Optional[float] = None\n",
    "  norm_eps: float = 1e-5\n",
    "  max_batch_size: int = 32\n",
    "  max_seq_len: int = 2048 # 1024\n",
    "  img_len: int = 577\n",
    "  img_dim: int = 1024\n",
    "  max_gen_len: int = 32\n",
    "\n",
    "class Tokenizer:\n",
    "  def __init__(self, model_path: str):\n",
    "    assert os.path.isfile(model_path), model_path\n",
    "    self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "    self.n_words: int = self.sp_model.vocab_size()\n",
    "    self.bos_id: int = self.sp_model.bos_id()\n",
    "    self.eos_id: int = self.sp_model.eos_id()\n",
    "    self.pad_id: int = self.sp_model.pad_id()\n",
    "    assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "  def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "    assert type(s) is str\n",
    "    t = self.sp_model.encode(s)\n",
    "    if bos:\n",
    "      t = [self.bos_id] + t\n",
    "    if eos:\n",
    "      t = t + [self.eos_id]\n",
    "    return t\n",
    "\n",
    "  def decode(self, t: List[int]) -> str:\n",
    "    return self.sp_model.decode(t)    \n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "  def __init__(self, dim: int, eps: float = 1e-6):\n",
    "    super().__init__()\n",
    "    self.eps = eps\n",
    "    self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "  def _norm(self, x):\n",
    "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self._norm(x.float()).type_as(x)\n",
    "    return output * self.weight\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, args):\n",
    "    super().__init__()\n",
    "    self.w1 = nn.Linear(args.img_dim, args.dim, bias=True).to(dtype=torch.float16)\n",
    "    self.gelu = nn.GELU()\n",
    "    self.w2 = nn.Linear(args.dim, args.dim, bias=True).to(dtype=torch.float16)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = self.w1(x)\n",
    "    h = self.gelu(h)\n",
    "    h = self.w2(h)\n",
    "    return h\n",
    "\n",
    "class Attention(nn.Module):\n",
    "  def __init__(self, args):\n",
    "    super().__init__()\n",
    "    self.n_heads = args.n_heads\n",
    "    self.head_dim = args.dim // args.n_heads\n",
    "    # print('self.head_dim', self.head_dim)\n",
    "    self.wq = nn.Linear(args.dim, self.n_heads * self.head_dim, bias=False)\n",
    "    self.wk = nn.Linear(args.dim, self.n_heads * self.head_dim, bias=False)\n",
    "    self.wv = nn.Linear(args.dim, self.n_heads * self.head_dim, bias=False)\n",
    "    self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)  \n",
    "\n",
    "  def reshape_for_broadcast(self, freqs_cis, x):\n",
    "    ndim = x.ndim\n",
    "    # print('x.shape', x.shape)\n",
    "    # print('ndim', ndim)\n",
    "    assert 0 <= 1 < ndim\n",
    "    # print('freqs_cis.shape', freqs_cis.shape)\n",
    "    # print('x.shape[1]', x.shape[1])\n",
    "    # print('x.shape[-1]', x.shape[-1])\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "  def apply_rotary_emb(self, xq, xk, freqs_cis):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    # print('xq_', xq_.shape)\n",
    "    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "  def forward(self, x, freqs_cis, mask=None):\n",
    "    # print('Attention.forward', x.shape, x.device, x.dtype)\n",
    "    # print('Attention.forward freqs_cis', freqs_cis.shape)\n",
    "    bsz, seqlen, _ = x.shape\n",
    "    # apply linear layer with checkpointing\n",
    "    xq = checkpoint(lambda x: self.wq(x), x)\n",
    "    # print('xq.shape', xq.shape)\n",
    "    xk = checkpoint(lambda x: self.wk(x), x)\n",
    "    xv = checkpoint(lambda x: self.wv(x), x)\n",
    "    # reshape to heads and head dims\n",
    "    xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "    # print('xq.shape', xq.shape)\n",
    "    xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "    xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "    # apply rotary encoding\n",
    "    xq, xk = self.apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "    # transpose heads to 2D for easier computation\n",
    "    xq = xq.transpose(1, 2)\n",
    "    xk = xk.transpose(1, 2)\n",
    "    xv = xv.transpose(1, 2)\n",
    "    # dot product of q v, scaled by the sqrt of head dims\n",
    "    dot_product = torch.matmul(xq, xk.transpose(2, 3))\n",
    "    scores = dot_product / math.sqrt(self.head_dim)\n",
    "    # apply masks\n",
    "    if mask is not None:\n",
    "      scores = scores + mask\n",
    "    # softmax to get attn scores\n",
    "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "    # get weighted sum from scores and value\n",
    "    output = torch.matmul(scores, xv)\n",
    "    # reshape back to original shape, apply linear layer\n",
    "    output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "    return self.wo(output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, dim, hidden_dim, multiple_of, ffn_dim_multiplier=None):\n",
    "    super().__init__()\n",
    "    hidden_dim = int(2 * hidden_dim / 3)\n",
    "    if ffn_dim_multiplier is not None:\n",
    "      hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "    self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "    self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "    self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = checkpoint(lambda x: F.silu(self.w1(x)) * self.w3(x), x)\n",
    "    return self.w2(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, layer_id: int, args):\n",
    "    super().__init__()\n",
    "    self.n_heads = args.n_heads\n",
    "    self.dim = args.dim\n",
    "    self.head_dim = args.dim // args.n_heads\n",
    "    self.attention = Attention(args)\n",
    "    self.feed_forward = FeedForward(\n",
    "      dim=args.dim,\n",
    "      hidden_dim=4 * args.dim,\n",
    "      multiple_of=args.multiple_of,\n",
    "      ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "    )\n",
    "    self.layer_id = layer_id\n",
    "    self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "    self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "  def forward(self, x, freqs_cis, mask=None, ):\n",
    "    # print('TransformerBlock.forward', x.shape, x.device, x.dtype)\n",
    "    h = x + self.attention.forward(\n",
    "      self.attention_norm(x), freqs_cis, mask\n",
    "    )\n",
    "    out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "    return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self.clip, _ = load(\"ViT-L/14@336px\")\n",
    "    self.mlp = MLP(params)\n",
    "    self.params = params\n",
    "    self.vocab_size = params.vocab_size\n",
    "    self.n_layers = params.n_layers\n",
    "    self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "    self.layers = torch.nn.ModuleList()\n",
    "    self.freqs_cis = self.precompute_freqs_cis(\n",
    "      self.params.dim // self.params.n_heads, \n",
    "      self.params.max_seq_len * 2\n",
    "    )\n",
    "    # print('self.params.dim', self.params.dim)\n",
    "    # print('self.params.n_heads', self.params.n_heads)\n",
    "    # print('self.params.max_seq_len * 2', self.params.max_seq_len * 2)\n",
    "    # print('self.freqs_cis', self.freqs_cis.shape)\n",
    "    \n",
    "    for layer_id in range(params.n_layers):\n",
    "      self.layers.append(TransformerBlock(layer_id, params))\n",
    "    self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "    self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "  def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
    "    indices = torch.arange(0, dim, 2) \n",
    "    sliced_indices = indices[: (dim // 2)].float()\n",
    "    scaled_indices = sliced_indices / dim\n",
    "    theta_power = theta ** scaled_indices\n",
    "    freqs = 1.0 / theta_power\n",
    "    t = torch.arange(end, device=\"cuda\")\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "  # @torch.inference_mode()\n",
    "  def forward(self, toks, imgs=None): \n",
    "    # image = None\n",
    "    _bsz, seqlen = toks.shape\n",
    "    # print('seqlen', seqlen)\n",
    "      \n",
    "    h = self.tok_embeddings(toks)\n",
    "    if imgs != None:\n",
    "      image_encoded = self.clip.encode_image(imgs).to(h.device)#, dtype=torch.float16)\n",
    "      image_encoded.detach()\n",
    "      # print('image_encoded.shape', image_encoded.shape)\n",
    "      image_projected = self.mlp(image_encoded)\n",
    "      # print('image_encoded.shape', image_encoded.shape)\n",
    "      image_projected = image_projected.expand(_bsz, -1, -1)\n",
    "      # print('image_encoded.shape', image_encoded.shape)\n",
    "      seqlen += image_projected.size(1)\n",
    "      # print('seqlen', seqlen)\n",
    "      h_before = h[:, :1, :]\n",
    "      h_after = h[:, 1:, :]\n",
    "      h = torch.cat([h_before, image_projected, h_after], dim=1)\n",
    "      # print('h.shape', h.shape)\n",
    "\n",
    "    \n",
    "    self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "    # print('seqlen', seqlen)\n",
    "    # print('self.freqs_cis', self.freqs_cis.shape)  \n",
    "    freqs_cis = self.freqs_cis[:seqlen]\n",
    "    # print('freqs_cis', freqs_cis.shape)\n",
    "    mask = None\n",
    "    \n",
    "    if seqlen > 1:\n",
    "      mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=toks.device)\n",
    "      mask = torch.triu(mask, diagonal=1)\n",
    "      mask = torch.hstack([\n",
    "        torch.zeros((seqlen, 0), device=toks.device), mask\n",
    "      ]).type_as(h)\n",
    "    \n",
    "    for layer in self.layers:\n",
    "      # print('h.shape', h.shape)\n",
    "      # print('freqs_cis.shape', freqs_cis.shape)\n",
    "      h = layer(h, freqs_cis, mask)\n",
    "    h = self.norm(h)\n",
    "    output = self.output(h).float()\n",
    "    return output\n",
    "\n",
    "\n",
    "# model utils\n",
    "\n",
    "def register_hooks(module, module_name_prefix='', sub=False):\n",
    "  def forward_hook(module, input, output):\n",
    "    log_message = f\"\\n{module_name_prefix} - forward:\\n\"\n",
    "    log_message += f\"Layer: {module.__class__.__name__}\\n\"\n",
    "    if hasattr(input[0], 'shape'):\n",
    "      log_message += f\"Input shape: {input[0].shape}\\n\"\n",
    "    if hasattr(output, 'shape'):\n",
    "      log_message += f\"Output shape: {output.shape}\\n\"\n",
    "    if hasattr(module, 'weight'):\n",
    "      log_message += f\"Weight mean: {module.weight.data.mean()}, std: {module.weight.data.std()}\\n\"\n",
    "    logging.info(log_message)\n",
    "\n",
    "  def backward_hook(module, grad_input, grad_output):\n",
    "    log_message = f\"{module_name_prefix} - backward:\\n\"\n",
    "    logging.info(log_message)\n",
    "\n",
    "  module.register_forward_hook(forward_hook)\n",
    "  module.register_full_backward_hook(backward_hook)\n",
    "\n",
    "  if sub:\n",
    "    for name, child in module.named_children():\n",
    "      new_prefix = f\"{module_name_prefix}.{name}\" if module_name_prefix else name\n",
    "      register_hooks(child, new_prefix, sub)\n",
    "\n",
    "def build(model_args):\n",
    "  start_time = time.time()\n",
    "  torch.cuda.set_device(0)\n",
    "  torch.set_default_tensor_type(torch.cuda.HalfTensor)  \n",
    "  ckpt = torch.load(\"consolidated.00.pth\", map_location=\"cuda\")\n",
    "  model = Transformer(model_args)\n",
    "  model.load_state_dict(ckpt, strict=False)\n",
    "  print(f\"llama loaded in {time.time() - start_time:.2f} seconds\")\n",
    "  return model\n",
    "\n",
    "def generate(model, tkzr, model_args, tkns, image=None, max_gen=32):\n",
    "  start_time = time.time()\n",
    "  bsz = len(tkns)\n",
    "  if max_gen:\n",
    "    max_gen_len = max_gen\n",
    "  else:  \n",
    "    max_gen_len = model_args.max_seq_len - 1\n",
    "  min_tkn_len = min(len(t) for t in tkns)\n",
    "  max_tkn_len = max(len(t) for t in tkns)\n",
    "  ttl_len = min(model_args.max_seq_len, max_gen_len + max_tkn_len)\n",
    "\n",
    "  pad_id = tkzr.pad_id\n",
    "  gen_tkns = torch.full((bsz, ttl_len), pad_id, dtype=torch.long)\n",
    "  for k, t in enumerate(tkns):\n",
    "    gen_tkns[k, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "  eos_reached = torch.tensor([False] * bsz)\n",
    "  input_text_mask = gen_tkns != pad_id\n",
    "  \n",
    "  for cur_pos in range(min_tkn_len, ttl_len):\n",
    "    logits = model.forward(gen_tkns[:, :cur_pos])#, image)\n",
    "    nxt_tkn = torch.argmax(logits[:, -1], dim=-1)\n",
    "    nxt_tkn = nxt_tkn.reshape(-1)\n",
    "    nxt_tkn = torch.where(input_text_mask[:, cur_pos], gen_tkns[:, cur_pos], nxt_tkn)\n",
    "    gen_tkns[:, cur_pos] = nxt_tkn\n",
    "    eos_reached |= (~input_text_mask[:, cur_pos]) & (nxt_tkn == tkzr.eos_id)\n",
    "    if all(eos_reached): break\n",
    "\n",
    "  out_tkns = []\n",
    "  for i, t in enumerate(gen_tkns.tolist()):\n",
    "    tkns_len = len(tkns[i])\n",
    "    t = t[tkns_len : tkns_len + max_gen_len]\n",
    "    if tkzr.eos_id in t:\n",
    "      eos_idx = t.index(tkzr.eos_id)\n",
    "      t = t[:eos_idx]\n",
    "    out_tkns.append(t)  \n",
    "  print(f\"generated in {time.time() - start_time:.2f} seconds\")\n",
    "  return out_tkns\n",
    "\n",
    "def test(model, tkzr, model_args):\n",
    "  prompts = {\n",
    "    'txt' : [\n",
    "      \"Simply put, the theory of relativity states that\", # the laws of physics are the same for all non-accelerating observers, regardless of their state of motion or their energy content.\n",
    "      \"Long ago there lived a magical cat named Puss\" # in Boots. Puss in Boots was a very clever cat. He was so clever that he could talk. He was so clever that he could talk\n",
    "    ],\n",
    "    'img' : [\"Simply put, the theory of relativity states that\"],\n",
    "    'train' : [\n",
    "      \"image label:\", \"image label:\",\n",
    "      \"photo description:\", \"photo description:\",\n",
    "      \"image title:\", \"image title:\",\n",
    "      \"picture summary:\", \"picture summary:\",\n",
    "    ]\n",
    "  }\n",
    "  tkns = [tkzr.encode(x, bos=True, eos=False) for x in prompts['txt']]\n",
    "  text_out_tkns = generate(model, tkzr, model_args, tkns, image=None)\n",
    "  [print(tkzr.decode(t)) for t in text_out_tkns]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53514bf4-d718-4e3e-94ab-e388881ead33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c4022e-c4ed-480a-9ccf-b2002cbbe4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "  def __init__(self, seq_len=32, bsz=4, shuffle=True):\n",
    "    self.clip, self.image_pre = load(\"ViT-L/14@336px\")\n",
    "    self.tkzr = Tokenizer('tokenizer.model')\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "    self.prompts = [\"image label: \", \"photo description: \",\"image title: \", \"picture summary: \"]\n",
    "\n",
    "    with open('data/metadata.json', 'r') as file: \n",
    "      data = json.load(file)\n",
    "    ds = [x for x in data if x.get('image') and x.get('blip_caption')]\n",
    "    if shuffle: random.shuffle(ds)\n",
    "    self.bsz_mult = 3\n",
    "    self.bsz = bsz * self.bsz_mult\n",
    "    ds = [ds[i:i + self.bsz] for i in range(0, len(ds) - len(ds) % self.bsz)] \n",
    "    self.ds = ds\n",
    "    self.index = 0\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.ds)\n",
    "\n",
    "  def __iter__(self):\n",
    "    self.index = 0\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self.index < len(self.ds):\n",
    "      result = self.__getitem__(self.index)\n",
    "      self.index += 1\n",
    "      return result\n",
    "    else:\n",
    "      raise StopIteration\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch = self.ds[idx]\n",
    "    text_toks = [self.tkzr.encode(d['blip_caption'], bos=False, eos=False) for d in batch]\n",
    "    min_len = min(len(x) for x in text_toks)\n",
    "    # print('min_len', min_len)\n",
    "\n",
    "    _images = []\n",
    "    _text = []\n",
    "    _target = []\n",
    "    for data in batch:\n",
    "      if len(_images) == self.bsz / self.bsz_mult:\n",
    "        break\n",
    "      img = Image.open(f\"data/images/{data['image']}\")\n",
    "      img = self.image_pre(img).unsqueeze(0).to(\"cuda\")\n",
    "      _images.append(img)\n",
    "\n",
    "      txt = data['blip_caption']\n",
    "      prefix = random.choice(self.prompts)\n",
    "      txt = prefix + txt\n",
    "      txt = self.tkzr.encode(data['blip_caption'], bos=True, eos=False)\n",
    "      txt = txt[: min_len]\n",
    "      _target.append(txt[-1])\n",
    "      txt = txt[:-1]\n",
    "      txt = torch.tensor(txt, dtype=torch.long, device=\"cuda\")\n",
    "      _text.append(txt)\n",
    "      \n",
    "    tgt = torch.tensor(_target, dtype=torch.long, device=\"cuda\")\n",
    "    src_img = torch.cat(_images, dim=0)\n",
    "    src_txt = torch.stack(_text, dim=0)\n",
    "    return src_txt, src_img, tgt\n",
    "# ds = Dataset(bsz=4)\n",
    "# src_txt, src_img, tgt = ds[0]\n",
    "# print('src_txt, src_img, tgt', src_txt.shape, src_img.shape, tgt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867569b-ea4f-4b74-b464-f2ad79e27a9d",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03028d4e-344a-4b3e-8c85-822229f783e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_len=10, bsz=4, accum=2):  \n",
    "  optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  gradient_accumulators = {}\n",
    "  \n",
    "  for n in range(1, train_len):\n",
    "    # src_txt, src_img, tgt = ds[n]\n",
    "    src_txt, src_img, tgt = ds[2]\n",
    "    # print('src_txt, src_img, tgt', src_txt.shape, src_img.shape, tgt.shape)\n",
    "    logits = model.forward(src_txt, src_img)\n",
    "    split_idx = src_txt.shape[1] - 1\n",
    "    if split_idx == 0: continue\n",
    "    logits = logits[:, -split_idx:, :]\n",
    "    logits = logits.reshape(-1, logits.size(-1))\n",
    "    \n",
    "    tgt_prefix = src_txt[:, 2:]\n",
    "    tgt = tgt.unsqueeze(1)\n",
    "    tgt = torch.cat((tgt_prefix, tgt), dim=1)\n",
    "    tgt = tgt.reshape(-1)\n",
    "    \n",
    "    loss = loss_fn(logits, tgt)\n",
    "    loss_value = loss.item()\n",
    "    loss = loss / accum\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # # accumulate gradients for each layer\n",
    "    # for name, param in model.named_parameters():\n",
    "    #   if param.requires_grad:\n",
    "    #     if \"weight\" in name:\n",
    "    #       if name not in gradient_accumulators:\n",
    "    #         gradient_accumulators[name] = param.grad.clone()\n",
    "    #       else:\n",
    "    #         gradient_accumulators[name] += param.grad\n",
    "    \n",
    "    # if n % accum == 0:\n",
    "      # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'\\n{n}\\t{loss_value}')\n",
    "      \n",
    "      # display accumulated gradients\n",
    "      # for name, accumulated_grad in gradient_accumulators.items():\n",
    "      #   print(f\"{name}: {accumulated_grad.norm().item()}\")  \n",
    "      # gradient_accumulators = {}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cd11a-bfcd-4696-a245-d6a2e868dbb8",
   "metadata": {},
   "source": [
    "# workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78c9de1-8c56-473e-a04b-95fd59a8ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip, img_pre = load(\"ViT-L/14@336px\")\n",
    "tkzr = Tokenizer(\"tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7622a1b3-70d7-4378-8221-7ec99a1fc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(bsz=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b4e180-1967-4604-87ab-3db25c11a45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama loaded in 24.29 seconds\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArgs(max_batch_size=4)\n",
    "model = build(model_args)\n",
    "# test(model, tkzr, model_args)\n",
    "# train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b1bd475-a964-4e4f-8373-2ce3158ea2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\t9.245932579040527\n",
      "\n",
      "2\t6.791743755340576\n",
      "\n",
      "3\t7.7349629402160645\n",
      "\n",
      "4\t7.402881145477295\n",
      "\n",
      "5\t5.4065985679626465\n",
      "\n",
      "6\t4.770587921142578\n",
      "\n",
      "7\t3.9755375385284424\n",
      "\n",
      "8\t3.9529778957366943\n",
      "\n",
      "9\t3.486147165298462\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
