{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c8c87a-280c-4e2d-a5e2-b5b84a668508",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6831eb43-8979-4bc0-936a-3386c92ae175",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "import torch.autograd.profiler as profiler\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from clip import load\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='log.txt', level=logging.INFO, filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20d0ed-17a1-4e9a-a218-bf31af0abd34",
   "metadata": {},
   "source": [
    "# model secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e0b126-4219-4671-bdf4-76a6646e3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "  dim: int = 4096\n",
    "  n_layers: int = 32\n",
    "  n_heads: int = 32\n",
    "  n_kv_heads: Optional[int] = None\n",
    "  vocab_size: int = 32000\n",
    "  multiple_of: int = 256  \n",
    "  ffn_dim_multiplier: Optional[float] = None\n",
    "  norm_eps: float = 1e-5\n",
    "  max_batch_size: int = 32\n",
    "  max_seq_len: int = 2048 # 1024\n",
    "  img_len: int = 577\n",
    "  img_dim: int = 1024\n",
    "  max_gen_len: int = 32\n",
    "\n",
    "class Tokenizer:\n",
    "  def __init__(self, model_path: str):\n",
    "    assert os.path.isfile(model_path), model_path\n",
    "    self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "    self.n_words: int = self.sp_model.vocab_size()\n",
    "    self.bos_id: int = self.sp_model.bos_id()\n",
    "    self.eos_id: int = self.sp_model.eos_id()\n",
    "    self.pad_id: int = self.sp_model.pad_id()\n",
    "    assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "  def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "    assert type(s) is str\n",
    "    t = self.sp_model.encode(s)\n",
    "    if bos:\n",
    "      t = [self.bos_id] + t\n",
    "    if eos:\n",
    "      t = t + [self.eos_id]\n",
    "    return t\n",
    "\n",
    "  def decode(self, t: List[int]) -> str:\n",
    "    return self.sp_model.decode(t)    \n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "  def __init__(self, dim: int, eps: float = 1e-6):\n",
    "    super().__init__()\n",
    "    self.eps = eps\n",
    "    self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "  def _norm(self, x):\n",
    "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self._norm(x.float()).type_as(x)\n",
    "    return output * self.weight\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, args):\n",
    "    super().__init__()\n",
    "    self.w1 = nn.Linear(args.img_dim, args.dim, bias=True).to(dtype=torch.float16)\n",
    "    self.gelu = nn.GELU()\n",
    "    self.w2 = nn.Linear(args.dim, args.dim, bias=True).to(dtype=torch.float16)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = self.w1(x)\n",
    "    h = self.gelu(h)\n",
    "    h = self.w2(h)\n",
    "    return h\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, dim, hidden_dim, multiple_of, ffn_dim_multiplier=None):\n",
    "    super().__init__()\n",
    "    hidden_dim = int(2 * hidden_dim / 3)\n",
    "    if ffn_dim_multiplier is not None:\n",
    "      hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "    self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "    self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "    self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = checkpoint(lambda x: F.silu(self.w1(x)) * self.w3(x), x)\n",
    "    return self.w2(x)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18509c-a463-48f4-99a4-6e425e59fab7",
   "metadata": {},
   "source": [
    "# model primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c096202-28e7-4a4a-a778-fe917982c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, args):\n",
    "    super().__init__()\n",
    "    self.n_heads = args.n_heads\n",
    "    self.head_dim = args.dim // args.n_heads\n",
    "    # print('self.head_dim', self.head_dim)\n",
    "    self.wq = nn.Linear(args.dim, self.n_heads * self.head_dim, bias=False)\n",
    "    self.wk = nn.Linear(args.dim, self.n_heads * self.head_dim, bias=False)\n",
    "    self.wv = nn.Linear(args.dim, self.n_heads * self.head_dim, bias=False)\n",
    "    self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)  \n",
    "\n",
    "  def reshape_for_broadcast(self, freqs_cis, x):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "  def apply_rotary_emb(self, xq, xk, freqs_cis):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "  def forward(self, x, freqs_cis, mask=None):\n",
    "    bsz, seqlen, _ = x.shape\n",
    "    # apply linear layer with checkpointing\n",
    "    xq = checkpoint(lambda x: self.wq(x), x)\n",
    "    # print('xq.shape', xq.shape)\n",
    "    xk = checkpoint(lambda x: self.wk(x), x)\n",
    "    xv = checkpoint(lambda x: self.wv(x), x)\n",
    "    # reshape to heads and head dims\n",
    "    xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "    # print('xq.shape', xq.shape)\n",
    "    xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "    xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "    # apply rotary encoding\n",
    "    xq, xk = self.apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "    # transpose heads to 2D for easier computation\n",
    "    xq = xq.transpose(1, 2)\n",
    "    xk = xk.transpose(1, 2)\n",
    "    xv = xv.transpose(1, 2)\n",
    "    # dot product of q v, scaled by the sqrt of head dims\n",
    "    dot_product = torch.matmul(xq, xk.transpose(2, 3))\n",
    "    scores = dot_product / math.sqrt(self.head_dim)\n",
    "    # apply masks\n",
    "    if mask is not None:\n",
    "      scores = scores + mask\n",
    "    # softmax to get attn scores\n",
    "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "    # get weighted sum from scores and value\n",
    "    output = torch.matmul(scores, xv)\n",
    "    # reshape back to original shape, apply linear layer\n",
    "    output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "    return self.wo(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf27d66-18b9-47b8-850e-b8be25c6b1b0",
   "metadata": {},
   "source": [
    "# transformer, block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e7941b-3aec-4872-a9ed-ff5c504f183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, layer_id: int, args):\n",
    "    super().__init__()\n",
    "    self.n_heads = args.n_heads\n",
    "    self.dim = args.dim\n",
    "    self.head_dim = args.dim // args.n_heads\n",
    "    self.attention = Attention(args)\n",
    "    self.feed_forward = FeedForward(\n",
    "      dim=args.dim,\n",
    "      hidden_dim=4 * args.dim,\n",
    "      multiple_of=args.multiple_of,\n",
    "      ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "    )\n",
    "    self.layer_id = layer_id\n",
    "    self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "    self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "  def forward(self, x, freqs_cis, mask=None, ):\n",
    "    h = x + self.attention.forward(\n",
    "      self.attention_norm(x), freqs_cis, mask\n",
    "    )\n",
    "    out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "    return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self.clip, _ = load(\"ViT-L/14@336px\")\n",
    "    self.mlp = MLP(params)\n",
    "    self.params = params\n",
    "    self.vocab_size = params.vocab_size\n",
    "    self.n_layers = params.n_layers\n",
    "    self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "    self.layers = torch.nn.ModuleList()\n",
    "    self.freqs_cis = self.precompute_freqs_cis(\n",
    "      self.params.dim // self.params.n_heads, \n",
    "      self.params.max_seq_len * 2\n",
    "    )\n",
    "    \n",
    "    for layer_id in range(params.n_layers):\n",
    "      self.layers.append(TransformerBlock(layer_id, params))\n",
    "    self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "    self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "  def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
    "    indices = torch.arange(0, dim, 2) \n",
    "    sliced_indices = indices[: (dim // 2)].float()\n",
    "    scaled_indices = sliced_indices / dim\n",
    "    theta_power = theta ** scaled_indices\n",
    "    freqs = 1.0 / theta_power\n",
    "    t = torch.arange(end, device=\"cuda\")\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "  # @torch.inference_mode()\n",
    "  def forward(self, toks, imgs=None): \n",
    "    _bsz, seqlen = toks.shape\n",
    "    h = self.tok_embeddings(toks)\n",
    "    \n",
    "    if imgs != None:\n",
    "      image_encoded = self.clip.encode_image(imgs).to(h.device)\n",
    "      image_encoded.detach()\n",
    "      image_projected = self.mlp(image_encoded)\n",
    "      image_projected = image_projected.expand(_bsz, -1, -1)\n",
    "      seqlen += image_projected.size(1)\n",
    "      \n",
    "      h_before = h[:, :1, :]\n",
    "      h_after = h[:, 1:, :]\n",
    "      h = torch.cat([h_before, image_projected, h_after], dim=1)\n",
    "    \n",
    "    self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "    freqs_cis = self.freqs_cis[:seqlen]\n",
    "    mask = None\n",
    "    \n",
    "    if seqlen > 1:\n",
    "      mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=toks.device)\n",
    "      mask = torch.triu(mask, diagonal=1)\n",
    "      mask = torch.hstack([\n",
    "        torch.zeros((seqlen, 0), device=toks.device), mask\n",
    "      ]).type_as(h)\n",
    "    \n",
    "    for layer in self.layers:\n",
    "      h = layer(h, freqs_cis, mask)\n",
    "    h = self.norm(h)\n",
    "    output = self.output(h).float()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2447c-7882-40fd-a53c-d6e25c7ec864",
   "metadata": {},
   "source": [
    "# hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "011e4506-64e0-43be-a4a6-8187e225e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_act_hook(module, layer_filter=None):\n",
    "  activations = {}\n",
    "  def hook(module, input, output):\n",
    "    if module not in activations:\n",
    "      ACT_DICT[module] = output.detach()\n",
    "    return hook   \n",
    "  for name, module in model.named_modules():\n",
    "    if layer_filter:\n",
    "      if layer_filter in name:\n",
    "        module.register_forward_hook(register_hook(module))\n",
    "    else:\n",
    "      module.register_forward_hook(register_hook(module))\n",
    "  return activations\n",
    "\n",
    "def register_mem_hooks(module, module_name_prefix='', sub=False):\n",
    "  def forward_hook(module, input, output):\n",
    "    log_message = f\"\\n{module_name_prefix} - forward:\\n\"\n",
    "    log_message += f\"Layer: {module.__class__.__name__}\\n\"\n",
    "    if hasattr(input[0], 'shape'):\n",
    "      log_message += f\"Input shape: {input[0].shape}\\n\"\n",
    "    if hasattr(output, 'shape'):\n",
    "      log_message += f\"Output shape: {output.shape}\\n\"\n",
    "    if hasattr(module, 'weight'):\n",
    "      log_message += f\"Weight mean: {module.weight.data.mean()}, std: {module.weight.data.std()}\\n\"\n",
    "    logging.info(log_message)\n",
    "\n",
    "  def backward_hook(module, grad_input, grad_output):\n",
    "    log_message = f\"{module_name_prefix} - backward:\\n\"\n",
    "    logging.info(log_message)\n",
    "\n",
    "  module.register_forward_hook(forward_hook)\n",
    "  module.register_full_backward_hook(backward_hook)\n",
    "\n",
    "  if sub:\n",
    "    for name, child in module.named_children():\n",
    "      new_prefix = f\"{module_name_prefix}.{name}\" if module_name_prefix else name\n",
    "      register_mem_hooks(child, new_prefix, sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f7bb0-dffb-4874-aaa2-c682c9542f9c",
   "metadata": {},
   "source": [
    "# build, generate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e407c7b-da62-4024-884c-6eca2028aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(model_args):\n",
    "  start_time = time.time()\n",
    "  torch.cuda.set_device(0)\n",
    "  torch.set_default_tensor_type(torch.cuda.HalfTensor)  \n",
    "  ckpt = torch.load(\"consolidated.00.pth\", map_location=\"cuda\")\n",
    "  model = Transformer(model_args)\n",
    "  model.load_state_dict(ckpt, strict=False)\n",
    "  print(f\"llama loaded in {time.time() - start_time:.2f} seconds\")\n",
    "  return model\n",
    "\n",
    "def generate(model, tkzr, model_args, tkns, image=None, max_gen=32):\n",
    "  start_time = time.time()\n",
    "  bsz = len(tkns)\n",
    "  if max_gen:\n",
    "    max_gen_len = max_gen\n",
    "  else:  \n",
    "    max_gen_len = model_args.max_seq_len - 1\n",
    "  min_tkn_len = min(len(t) for t in tkns)\n",
    "  max_tkn_len = max(len(t) for t in tkns)\n",
    "  ttl_len = min(model_args.max_seq_len, max_gen_len + max_tkn_len)\n",
    "\n",
    "  pad_id = tkzr.pad_id\n",
    "  gen_tkns = torch.full((bsz, ttl_len), pad_id, dtype=torch.long)\n",
    "  for k, t in enumerate(tkns):\n",
    "    gen_tkns[k, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "  eos_reached = torch.tensor([False] * bsz)\n",
    "  input_text_mask = gen_tkns != pad_id\n",
    "  \n",
    "  for cur_pos in range(min_tkn_len, ttl_len):\n",
    "    logits = model.forward(gen_tkns[:, :cur_pos])#, image)\n",
    "    nxt_tkn = torch.argmax(logits[:, -1], dim=-1)\n",
    "    nxt_tkn = nxt_tkn.reshape(-1)\n",
    "    nxt_tkn = torch.where(input_text_mask[:, cur_pos], gen_tkns[:, cur_pos], nxt_tkn)\n",
    "    print(nxt_tkn)\n",
    "    gen_tkns[:, cur_pos] = nxt_tkn\n",
    "    eos_reached |= (~input_text_mask[:, cur_pos]) & (nxt_tkn == tkzr.eos_id)\n",
    "    if all(eos_reached): break\n",
    "\n",
    "  out_tkns = []\n",
    "  for i, t in enumerate(gen_tkns.tolist()):\n",
    "    tkns_len = len(tkns[i])\n",
    "    t = t[tkns_len : tkns_len + max_gen_len]\n",
    "    if tkzr.eos_id in t:\n",
    "      eos_idx = t.index(tkzr.eos_id)\n",
    "      t = t[:eos_idx]\n",
    "    out_tkns.append(t)  \n",
    "  print(f\"generated in {time.time() - start_time:.2f} seconds\")\n",
    "  return out_tkns\n",
    "\n",
    "def test(model, tkzr, model_args):\n",
    "  prompts = {\n",
    "    'txt' : [\n",
    "      \"Simply put, the theory of relativity states that\", # the laws of physics are the same for all non-accelerating observers, regardless of their state of motion or their energy content.\n",
    "      \"Long ago there lived a magical cat named Puss\" # in Boots. Puss in Boots was a very clever cat. He was so clever that he could talk. He was so clever that he could talk\n",
    "    ],\n",
    "    'img' : [\"Simply put, the theory of relativity states that\"],\n",
    "    'train' : [\n",
    "      \"image label:\", \"image label:\",\n",
    "      \"photo description:\", \"photo description:\",\n",
    "      \"image title:\", \"image title:\",\n",
    "      \"picture summary:\", \"picture summary:\",\n",
    "    ]\n",
    "  }\n",
    "  tkns = [tkzr.encode(x, bos=True, eos=False) for x in prompts['txt']]\n",
    "  text_out_tkns = generate(model, tkzr, model_args, tkns, image=None)\n",
    "  [print(tkzr.decode(t)) for t in text_out_tkns]    \n",
    "\n",
    "def train(model, train_len=1, bsz=4, accum=2):  \n",
    "  \n",
    "  for n in range(1, train_len + 1):\n",
    "    src_txt, src_img, tgt = ds[n]\n",
    "    \n",
    "    split_idx = src_txt.shape[1] - 1\n",
    "    if split_idx == 0: continue\n",
    "      \n",
    "    logits = model.forward(src_txt, src_img)\n",
    "    \n",
    "    logits = logits[:, -split_idx:, :]\n",
    "    logits = logits.reshape(-1, logits.size(-1))\n",
    "    \n",
    "    tgt_prefix = src_txt[:, 2:]\n",
    "    tgt = tgt.unsqueeze(1)\n",
    "    tgt = torch.cat((tgt_prefix, tgt), dim=1)\n",
    "    tgt = tgt.reshape(-1)\n",
    "    \n",
    "    loss = loss_fn(logits, tgt)\n",
    "    loss_value = loss.item()\n",
    "      \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'\\n{n}\\t{loss_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53514bf4-d718-4e3e-94ab-e388881ead33",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c4022e-c4ed-480a-9ccf-b2002cbbe4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "  def __init__(self, seq_len=32, bsz=4, shuffle=True):\n",
    "    self.clip, self.image_pre = load(\"ViT-L/14@336px\")\n",
    "    self.tkzr = Tokenizer('tokenizer.model')\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "    self.prompts = [\"image label: \", \"photo description: \",\"image title: \", \"picture summary: \"]\n",
    "\n",
    "    with open('data/metadata.json', 'r') as file: \n",
    "      data = json.load(file)\n",
    "    ds = [x for x in data if x.get('image') and x.get('blip_caption')]\n",
    "    if shuffle: random.shuffle(ds)\n",
    "    self.bsz_mult = 3\n",
    "    self.bsz = bsz * self.bsz_mult\n",
    "    ds = [ds[i:i + self.bsz] for i in range(0, len(ds) - len(ds) % self.bsz)] \n",
    "    self.ds = ds\n",
    "    self.index = 0\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.ds)\n",
    "\n",
    "  def __iter__(self):\n",
    "    self.index = 0\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self.index < len(self.ds):\n",
    "      result = self.__getitem__(self.index)\n",
    "      self.index += 1\n",
    "      return result\n",
    "    else:\n",
    "      raise StopIteration\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch = self.ds[idx]\n",
    "    text_toks = [self.tkzr.encode(d['blip_caption'], bos=False, eos=False) for d in batch]\n",
    "    min_len = min(len(x) for x in text_toks)\n",
    "    # print('min_len', min_len)\n",
    "\n",
    "    _images = []\n",
    "    _text = []\n",
    "    _target = []\n",
    "    for data in batch:\n",
    "      if len(_images) == self.bsz / self.bsz_mult:\n",
    "        break\n",
    "      img = Image.open(f\"data/images/{data['image']}\")\n",
    "      img = self.image_pre(img).unsqueeze(0).to(\"cuda\")\n",
    "      _images.append(img)\n",
    "\n",
    "      txt = data['blip_caption']\n",
    "      prefix = random.choice(self.prompts)\n",
    "      txt = prefix + txt\n",
    "      txt = self.tkzr.encode(data['blip_caption'], bos=True, eos=False)\n",
    "      txt = txt[: min_len]\n",
    "      _target.append(txt[-1])\n",
    "      txt = txt[:-1]\n",
    "      txt = torch.tensor(txt, dtype=torch.long, device=\"cuda\")\n",
    "      _text.append(txt)\n",
    "      \n",
    "    tgt = torch.tensor(_target, dtype=torch.long, device=\"cuda\")\n",
    "    src_img = torch.cat(_images, dim=0)\n",
    "    src_txt = torch.stack(_text, dim=0)\n",
    "    return src_txt, src_img, tgt\n",
    "# ds = Dataset(bsz=4)\n",
    "# src_txt, src_img, tgt = ds[0]\n",
    "# print('src_txt, src_img, tgt', src_txt.shape, src_img.shape, tgt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867569b-ea4f-4b74-b464-f2ad79e27a9d",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03028d4e-344a-4b3e-8c85-822229f783e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "675cd11a-bfcd-4696-a245-d6a2e868dbb8",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e78c9de1-8c56-473e-a04b-95fd59a8ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip, img_pre = load(\"ViT-L/14@336px\")\n",
    "TKZR = Tokenizer(\"tokenizer.model\")\n",
    "DS = Dataset(bsz=4)\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b4e180-1967-4604-87ab-3db25c11a45f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama loaded in 9.45 seconds\n"
     ]
    }
   ],
   "source": [
    "if 'model' in locals() or 'model' in globals():\n",
    "  del model\n",
    "  gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "MODEL_ARGS = ModelArgs(max_batch_size=4)\n",
    "MODEL = build(MODEL_ARGS)\n",
    "\n",
    "OPTIMIZER = optim.SGD(MODEL.parameters(), lr=1e-4, momentum=0.9)\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "for name, param in MODEL.named_parameters():\n",
    "  if 'clip' in name:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72348e-dc2b-4e37-a83f-bccf4b1301b4",
   "metadata": {},
   "source": [
    "# introspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f325e4b-86a4-49cc-9f6d-9517f22fb040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simply put, the theory of relativity states that\n",
      "last tok: [that]\n",
      "[that] \t 26.953125\n",
      "[,] \t 19.984375\n",
      "[the] \t 18.6875\n",
      "[there] \t 18.328125\n",
      "[:] \t 17.875\n",
      "[a] \t 17.84375\n",
      "\n",
      "Long ago there lived a magical cat named Puss\n",
      "last tok: [uss]\n",
      "[uss] \t 25.46875\n",
      "[us] \t 17.921875\n",
      "[ush] \t 16.625\n",
      "[urr] \t 16.40625\n",
      "[ete] \t 15.5625\n",
      "[ink] \t 15.171875\n"
     ]
    }
   ],
   "source": [
    "def debug(action, layer_filter=None):\n",
    "  model, tkzr, loss_fn, optimizer = MODEL, TKZR, LOSS_FN, OPTIMIZER\n",
    "  optimizer.zero_grad()\n",
    "    \n",
    "  activations, layer_names = [], []\n",
    "  def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "    layer_names.append(module.__class__.__name__)\n",
    "\n",
    "  hooks = []\n",
    "  if action == 'activation':\n",
    "    for name, layer in model.named_modules():\n",
    "      if layer_filter in name:\n",
    "        hook = layer.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "        \n",
    "  prompts = [\"Simply put, the theory of relativity states that\", \"Long ago there lived a magical cat named Puss\"]\n",
    "  toks = [tkzr.encode(x, bos=True, eos=False) for x in prompts]\n",
    "  toks = [torch.tensor(x) for x in toks]\n",
    "  toks = torch.stack(toks, dim=0)\n",
    "  src = toks[:,:-1]\n",
    "  tgt = toks[:,1:].reshape(-1)\n",
    "\n",
    "  logits = model.forward(src) \n",
    "  logits = logits.reshape(-1, logits.size(-1)) \n",
    "  loss = loss_fn(logits, tgt)\n",
    "  loss.backward()\n",
    "    \n",
    "  if action == 'logits': \n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(src) \n",
    "    # loss = loss_fn(logits, tgt) ; print(f'loss:\\t{loss.item()}')\n",
    "    # Logic for examining logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    \n",
    "    values, idxs = torch.topk(last_logits, 6, dim=1)  # Get top 6 logits and their indices\n",
    "\n",
    "    for i, seq in enumerate(idxs.tolist()):\n",
    "      print(f'\\n{tkzr.decode(toks[i].tolist())}')\n",
    "      print(f'last tok: [{tkzr.decode(toks[i].tolist()[-1:])}]')\n",
    "      for j, tok in enumerate(seq):\n",
    "        print(f'[{tkzr.decode([tok])}] \\t {values[i, j].item()}')\n",
    "    return\n",
    "      \n",
    "    # d = [tkzr.decode([i]) for i in   \n",
    "\n",
    "      \n",
    "    # decod = tkzr.decode(top_indices.tolist()[0]) ; print(decod) ; return\n",
    "    decod = tkzr.decode(top_indices.tolist()[1][2:3]) ; print(decod) ; return\n",
    "    decoded_tokens = [tkzr.decode(x) for x in top_indices.tolist()]\n",
    "\n",
    "  if action == 'overfit':\n",
    "    optimizer.zero_grad()\n",
    "    for n in range(1, 20):\n",
    "      logits = model.forward(src) \n",
    "      logits = logits.reshape(-1, logits.size(-1)) \n",
    "      loss = loss_fn(logits, tgt) ; print(f'{n}\\t{loss.item()}')\n",
    "      loss.backward()     \n",
    "      optimizer.step()\n",
    "          \n",
    "  if action in ['weight', 'gradient']:\n",
    "    print(f'i\\tmean\\t\\tstd\\t\\tlayer_name')  \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "      if param.grad is not None and (layer_filter is None or layer_filter in name):\n",
    "        data = param.data if viz_type == 'weight' else param.grad\n",
    "        print(f'{i}\\t{data.mean():+f}\\t{data.std():e}\\t{name}')\n",
    "        hy, hx = torch.histogram(data.cpu().float(), density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach()) ; plt.show()\n",
    "\n",
    "  if action == 'activation':\n",
    "    print(f'i\\tmean\\t\\tstd\\t\\tsat\\t\\tlayer_name')    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, activation in enumerate(activations):\n",
    "      act = activation.detach().cpu().float()\n",
    "      hy, hx = torch.histogram(act, density=True)\n",
    "      print(f'{i}\\t {layer_names[i]:10}, mean: {act.mean():+.4f}, std: {act.std():e}, saturated: {(act.abs() > 0.97).float().mean() * 100:.2f}%')\n",
    "      plt.plot(hx[:-1], hy) ; plt.show()\n",
    "    [hook.remove() for hook in hooks]\n",
    "\n",
    "  if action == 'weight_update':\n",
    "    optimizer.zero_grad()      \n",
    "    logits = model.forward(src) \n",
    "    logits = logits.reshape(-1, logits.size(-1)) \n",
    "    loss = loss_fn(logits, tgt) ; print(f'loss:\\t{loss.item()}')\n",
    "    loss.backward()     \n",
    "\n",
    "    pre_update_weights = {}\n",
    "    for name, param in model.named_parameters():\n",
    "      if (layer_filter is None or layer_filter in name) and param.grad is not None:\n",
    "        pre_update_weights[name] = param.clone()         \n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'i\\tmean\\t\\tstd\\t\\tlayer_name')          \n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "      if name in pre_update_weights:\n",
    "        update = param - pre_update_weights[name]\n",
    "        if param.grad is not None:  # Ensure there is a gradient\n",
    "          epsilon = 1e-8\n",
    "          normalized_update = update / (pre_update_weights[name].abs() + epsilon)\n",
    "          \n",
    "          mean_normalized_update = normalized_update.mean().item()\n",
    "          std_normalized_update = normalized_update.std().item()\n",
    "          \n",
    "          print(f'{i}\\t{mean_normalized_update:+.4e}\\t{std_normalized_update:e}\\t{name}')      \n",
    "          hy, hx = torch.histogram(normalized_update.cpu().float(), density=True)\n",
    "          plt.plot(hx[:-1].detach(), hy.detach(), label=name)        \n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "debug('logits')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
